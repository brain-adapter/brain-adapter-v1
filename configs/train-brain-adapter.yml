trainer:
  seed: 2024
  check_val_every_n_epoch: 1
  max_epochs: 5
  num_devices: 1

  checkpoint:
    dirpath: /root/autodl-tmp/ckpts
    monitor: step
    filename: brain-adapter-raw-{step}
    save_top_k: 5 # save all and conduct ablation studies
    mode: max
    every_n_epochs: 1
    save_last: false

# for Tensorboard logger
logger:
    save_dir: /root/tf-logs
    sub_dir: logs
    name: brain-adapter
    version: 5-95-train
    default_hp_metric: false

lightning:
  name: model.lightnings.LitBrainAdapterModel
  # employ pretrained vision adapter
  pretrained_model_path: /root/autodl-tmp/pretrained/vision-adapter/part1 # use part-1 that trained on 5,000,000 images
  diffusion_model_path: &diffusion_model_path /root/autodl-tmp/pretrained/stable-diffusion-v1-5

  snr_gamma: 5.0
  num_images_per_prompt: 4
  num_inference_steps: 50

  brain_model_path: /root/autodl-tmp/pretrained/brain-model-moe

model: null


dataset: 
  name: data.dataset.EEGImageNetDatasetForReconstruction
  num_workers: 2 # num_devices * 4
  resolution: 512
  batch_size: 4
  image_ext: JPEG
  subject: 0 # MOE is trained on all subjects and there is no need to finetune on a single subject

  eeg_data_path: /root/autodl-tmp/data/eeg-imagenet/eeg_5_95_std.pth
  splitter_path: /root/autodl-tmp/data/eeg-imagenet/block_splits_by_image_all.pth
  image_root_path: /root/autodl-tmp/data/eeg-imagenet/images

  clip_model_path: /root/autodl-tmp/pretrained/clip-vit-large-patch14-336
  diffusion_model_path: *diffusion_model_path

  merge_train_and_val: true

  drop_probability: 0.05 # for enhancing classifier-free guidance

# constant learning rate
optimizer:
  name: torch.optim.AdamW
  params:
    lr: 2.e-5 # finetune
    weight_decay: 0.01
