trainer:
  seed: 2024
  check_val_every_n_epoch: &interval 1
  max_epochs: &max_epochs 25
  num_devices: 1

  checkpoint:
    dirpath: checkpoints
    monitor: step
    filename: brain-model-raw-{step}
    save_top_k: 5
    mode: max
    every_n_epochs: *interval
    save_last: false
  
  resume_ckpt_path: null

# for Tensorboard logger
logger:
    save_dir: all-logs
    sub_dir: logs
    name: brain-model
    version: 5-95-train 
    default_hp_metric: false

lightning:
  name: model.lightnings.LitBrainModel

  pretrained_model_path: null
  clip_model_path: &clip_model_path pretrained/clip-vit-large-patch14-336 

  log_clip_logits: true # log similarity matrix within a batch during validation loop
  
  cls_ratio: 0.08 # ratio for cross entropy loss
  

model:
  name: model.models.TransformerEncoderModel
  encoder_name: model.modules.EEGMOETransformer

  num_channels: 128
  num_subjects: 6
  num_samples: 440
  patch_size: 4
  hidden_size: 768
  num_layers: 8 
  num_attention_heads: 12
  mlp_ratio: 1.0
  dropout: 0.5
  act_fn: gelu


dataset: 
  name: data.dataset.EEGImageNetFeaturesDataset # Dataset with features to accelarate the distillation process

  num_workers: 2 # num_devices * 2
  subject: 0 # 0 means using data of all subjects
  batch_size: 64 # better than 16
  num_classes: 40

  image_ext: JPEG

  eeg_data_path: eeg_5_95_std.pth
  splitter_path: block_splits_by_image_all.pth
  image_embeds_path: data/image_embeds.pth # output of `image2embeds.py`

  merge_train_and_val: true


optimizer:
  name: torch.optim.AdamW
  params:
    lr: 1.e-4 
    weight_decay: 0.05

scheduler:
  name: flash.core.optimizers.LinearWarmupCosineAnnealingLR
  params:
    warmup_epochs: 3
    max_epochs: *max_epochs
    warmup_start_lr: 4.e-5
    eta_min: 2.e-5
