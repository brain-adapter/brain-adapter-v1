trainer:
  task: train_only
  seed: 2024
  save_directory: /root/autodl-tmp/pretrained
  check_val_every_n_epoch: 1
  max_epochs: 30
  num_devices: 1

  checkpoint:
    dirpath: /root/autodl-tmp/ckpts
    monitor: val_loss
    filename: lit-{val_loss:.4f}
    save_top_k: 1
    mode: min
    every_n_epochs: 1
    save_last: false

# for Tensorboard logger
logger:
    save_dir: /root/tf-logs
    sub_dir: logs
    name: blur-reconstruction
    version: 0
    default_hp_metric: false

lightning:
  name: model.lightnings.LitBlurReconModel
  diffusion_model_path: &diffusion_model_path /root/autodl-tmp/pretrained/stable-diffusion-v1-5
  eeg_model_path: /root/autodl-tmp/pretrained/brain-kd-model-val_loss=0.2530

model:
  # Joint model
  resampler: 
    name: model.models.EncoderModel

    encoder_name: model.modules.PerceiverResampler
    output_key: 0
    # params
    input_dim: 768
    cross_attention_dim: 768
    hidden_size: 512
    num_layers: 4
    num_attention_heads: 8
    mlp_ratio: 1.0
    output_dim: 256
    query_tokens: 64
  
  decoder:
    name: model.models.ExternalModel
    model_name: diffusers.models.autoencoders.vae.Decoder
    params:
      in_channels: 256
      out_channels: 4
      up_block_types: 
        - UpDecoderBlock2D
        - UpDecoderBlock2D
        - UpDecoderBlock2D
        - UpDecoderBlock2D
    
      block_out_channels:
        - 64
        - 128
        - 256
        - 256

      layers_per_block: 1
    

dataset: 
  name: data.dataset.EEGImageNetDatasetForBlurReconstruction
  num_workers: 2 # num_devices * 2
  resolution: 512
  batch_size: 4
  subject: 0 # 0 means using data of all subjects

  image_ext: JPEG

  image_root_path: /root/autodl-tmp/data/eeg-imagenet/images
  eeg_data_path: /root/autodl-tmp/data/eeg-imagenet/eeg_5_95_std.pth
  splitter_path: /root/autodl-tmp/data/eeg-imagenet/block_splits_by_image_all.pth

  merge_train_and_val: true
  num_validation_images: 8


optimizer:
  name: torch.optim.AdamW
  params:
    lr: 1.e-4
    weight_decay: 0.05

scheduler:
  name: flash.core.optimizers.LinearWarmupCosineAnnealingLR
  params:
    warmup_epochs: 3
    max_epochs: 30
    warmup_start_lr: 5.e-5
    eta_min: 4.e-5