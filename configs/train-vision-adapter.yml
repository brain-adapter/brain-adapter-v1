trainer:
  seed: 2024
  val_check_interval: &interval 900
  max_epochs: 10
  num_devices: 4

  checkpoint:
    dirpath: checkpoints
    monitor: step
    filename: vision-adapter-{step}
    save_top_k: 3
    mode: max
    every_n_train_steps: *interval
    save_last: false
  
  resume_ckpt_path: null

# for Tensorboard logger
logger:
    save_dir: all-logs
    sub_dir: logs
    name: vision-adapter
    version: 0
    default_hp_metric: false

lightning:
  name: model.lightnings.LitVisionAdapterModel
  diffusion_model_path: &diffusion_model_path pretrained/stable-diffusion-v1-5
  
  snr_gamma: 5.0
  num_images_per_prompt: 4
  num_inference_steps: 30

  clip_model_path: &clip_model_path pretrained/clip-vit-large-patch14-336

model:
  diffusion_model_path: *diffusion_model_path

  projection_config:
    input_dim: 768
    num_tokens: 4

dataset: 
  name: data.dataset.ImageDataset
  num_workers: 8 # num_devices * 2
  resolution: 512
  batch_size: 8
  image_ext: jpg

  image_root_path: 
    train: data/laion-aesthetics/train
    val: data/laion-aesthetics/validation

  clip_model_path: *clip_model_path

  drop_probability: 0.05

# constant learning rate
optimizer:
  name: torch.optim.AdamW
  params:
    lr: 1.e-4
    weight_decay: 0.01
